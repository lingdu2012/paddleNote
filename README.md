# paddleNote
百度飞桨学习笔记

# 基础知识（一）

* 人工智能 > 机器学习 > 深度学习

* 机器学习的实现可以分成两步：训练（归纳：从具体案例中抽象一般规律）和预测（演绎：从一般规律推导出具体案例的结果）

* 模型有效的基本条件是能够拟合已知的样本

* 衡量模型预测值和真实值差距的评价函数也被称为损失函数（损失Loss）

* 最小化损失是模型的优化目标，实现损失最小化的方法称为优化算法，也称为寻解算法（找到使得损失函数最小的参数解）。

* 模型假设、评价函数（损失/优化目标）和优化算法是构成模型的三个部分。

* 机器执行学习的框架体现了其学习的本质是“参数估计”（Learning is parameter estimation）

* 机器学习和深度学习在理论结构上是一致的，即：模型假设、评价函数和优化算法，其根本差别在于假设的复杂度

* 神经元： 神经网络中每个节点称为神经元，由两部分组成：
  加权和：将所有输入加权求和。
  非线性变换（激活函数）：加权和的结果经过一个非线性函数变换，让神经元计算具备非线性的能力。
  多层连接： 大量这样的节点按照不同的层次排布，形成多层的结构连接起来，即称为神经网络。
  前向计算： 从输入计算输出的过程，顺序从网络前至后。
  计算图： 以图形化的方式展现神经网络的计算逻辑又称为计算图。

* 神经网络并没有那么神秘，它的本质是一个含有很多参数的“大公式”。

* SIFT，即尺度不变特征变换（Scale-invariant feature transform，SIFT），是用于图像处理领域的一种描述。这种描述具有尺度不变性，可在图像中检测出关键点，是一种局部特征描述子。

* 神经网络的标准结构中每个神经元由加权和与非线性变换构成，然后将多个神经元分层的摆放并连接形成神经网络。

  

# 基础知识（二）

* 构建神经网络/深度学习模型的基本步骤：数据处理、模型设计、训练配置、训练过程、模型保存
* 数据处理：数据导入、数据形状变换、数据集划分、数据归一化处理、封装load data 函数
* 数据集：训练集（确定模型的参数）和测试集（评判模型的效果）
* 数据归一化处理：使每个特征的取值缩放到0-1之间，可以使模型训练更高效，特征前的权重大小可以代表该变量对预测结果的贡献度
* 将数据处理的操作封装成 load data 函数有助于模型的调用
* 对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标；对于分类问题，通常采用交叉熵作为损失函数
* Loss=(y−z)2
* 模型训练的目标是让定义的损失函数尽可能的小，即取得极小值
* 基于Numpy广播机制，可以更快速地实现梯度计算
* 学习率：控制每次参数值沿着梯度反方向变动的大小，即每次移动的步长。
* 随机梯度下降法（Stochastic Gradient Descent，SGD）：每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数。
* 数据处理需要实现拆分数据批次和样本乱序（为了实现随机抽样的效果）两个功能。
* 当前大部分深度学习框架在建模过程中，每一步都可以分为两个部分：个性化（指定模型由哪些逻辑元素组合，由建模者完成）和通用化（聚焦这些元素的算法实现，由框架完成）